{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6be911f6-b8a8-44da-bd47-eb9b17762b32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All threads created.\n",
      "\n",
      "✅ Report saved at: /Users/denizdalkilic/Documents/Forks/PyRIT/tests/E2E/reports/DataSet/report_20250325_224941.html\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import asyncio\n",
    "import time\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# Pyrit imports\n",
    "from pyrit.common import IN_MEMORY, initialize_pyrit\n",
    "from pyrit.prompt_target import OpenAIChatTarget, HTTPTarget\n",
    "from pyrit.score.evaluator import Evaluator\n",
    "from pyrit.orchestrator import PromptSendingOrchestrator\n",
    "from pyrit.common.text_helper import save_html_report, generate_single_turn_html_report\n",
    "from pyrit.prompt_target.http_target.parsers.AHAssistantResponseParser import AHAssistantResponseParser\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "TOKEN = \"\"\n",
    "\n",
    "# Endpoint to create a new thread\n",
    "create_thread_url = \"\"\n",
    "\n",
    "# Endpoint for chat\n",
    "chat_url = \"\"\n",
    "\n",
    "# Headers for all requests\n",
    "HEADERS = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"X-Authorization\": TOKEN,\n",
    "    \"Accept\": \"*/*\"\n",
    "}\n",
    "\n",
    "# Payload for thread creation\n",
    "thread_init_payload = {\n",
    "    \"message\": \"Hello, start this chat!\",\n",
    "    \"model\": \"gpt-4o-mini\"\n",
    "}\n",
    "\n",
    "# HTTP template to send a prompt to a thread\n",
    "send_prompt_http_template = f\"\"\"\n",
    "    POST {chat_url}\n",
    "    Content-Type: application/json\n",
    "    X-Authorization: {TOKEN}\n",
    "    Accept: */*\n",
    "\n",
    "    {{\n",
    "        \"message\": \"{{PROMPT}}\"\n",
    "    }}\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# QA Dataset\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "qa_pairs = [\n",
    "    {\"question\": \"Ik heb zin in fruit\", \"expected_outcomes\": \"text_message: Vraag om meer details\"},\n",
    "    {\"question\": \"Appel\", \"expected_outcomes\": \"text_message: Apples\"}\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Initialize Pyrit Environment\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "initialize_pyrit(memory_db_type=IN_MEMORY)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Setup Pyrit Targets and Evaluator\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "http_prompt_target = HTTPTarget(\n",
    "    http_request=send_prompt_http_template,\n",
    "    prompt_regex_string=\"{PROMPT}\",\n",
    "    timeout=60.0,\n",
    "    callback_function=AHAssistantResponseParser.parse_response\n",
    ")\n",
    "\n",
    "scorer = Evaluator(\n",
    "    chat_target=OpenAIChatTarget(),\n",
    "    evaluator_yaml_path=Path(\"assets/AH_Evaluators/relevance_evaluator.yaml\"),\n",
    "    scorer_type=\"float_scale\"\n",
    ")\n",
    "\n",
    "orchestrator = PromptSendingOrchestrator(\n",
    "    objective_target=http_prompt_target,\n",
    "    scorers=[scorer]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Helper: Create Required Number of Chat Threads\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def create_chat_threads(required_count: int, delay_seconds: float = 0.5):\n",
    "    thread_ids = []\n",
    "    attempt = 0\n",
    "\n",
    "    while len(thread_ids) < required_count:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            response = requests.post(create_thread_url, headers=HEADERS, json=thread_init_payload)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            thread_id = response.json().get(\"chatId\")\n",
    "            if thread_id:\n",
    "                thread_ids.append(thread_id)\n",
    "            else:\n",
    "                print(f\"[Attempt {attempt}] No chatId in response.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Attempt {attempt}] Failed to create thread: {e}\")\n",
    "            try:\n",
    "                print(\"Raw response:\", response.text)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        time.sleep(delay_seconds)\n",
    "\n",
    "    print(\"✅ All threads created.\\n\")\n",
    "    return thread_ids\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Main Async Function\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "async def main():\n",
    "    questions = [pair[\"question\"] for pair in qa_pairs]\n",
    "    expected_outcomes = [pair[\"expected_outcomes\"] for pair in qa_pairs]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Create chat threads\n",
    "    thread_ids = create_chat_threads(required_count=len(qa_pairs))\n",
    "\n",
    "    # 2. Send prompts into those threads\n",
    "    await orchestrator.send_prompts_async(\n",
    "        prompt_list=questions,\n",
    "        expected_output_list=expected_outcomes,\n",
    "        thread_ids=thread_ids\n",
    "    )\n",
    "\n",
    "    # 3. Collect results and execution time\n",
    "    results = orchestrator.get_chat_results()\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # 4. Save HTML report\n",
    "    report_dir = Path(\"tests/E2E/reports/DataSet\").resolve()\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    save_html_report(\n",
    "        results=results,\n",
    "        directory=str(report_dir),\n",
    "        report_generator=generate_single_turn_html_report,\n",
    "        is_chat_evaluation=False,\n",
    "        threshold=0.7,\n",
    "        description=\"Evaluation of inputs vs. expected/actual outputs with scoring.\",\n",
    "        execution_time=execution_time\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Run It\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df9638a-5e9f-4760-b69e-ee3f2efcdaf3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e286d-08b0-4217-a02d-95a2c6a8308b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
