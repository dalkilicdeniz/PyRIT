{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b9c029-5329-4211-93e5-4b0d9acbe085",
   "metadata": {},
   "source": [
    "# üöÄ Automated QA Testing for Steijn Assistant using PyRIT\n",
    "\n",
    "## üìå Overview\n",
    "This notebook automates **QA testing** for the **Steijn Assistant** using the **PyRIT** framework. It sends predefined prompts to the assistant, evaluates its responses, and generates a report.\n",
    "\n",
    "## üõ†Ô∏è Steps in this Notebook\n",
    "- **üîß Setup Configuration** - Define API endpoints, authentication, and request templates.\n",
    "- **üìã Load QA Dataset** - Define test questions and expected answers.\n",
    "- **‚öôÔ∏è Initialize PyRIT** - Configure the testing environment.\n",
    "- **üì° Send Prompts & Evaluate Responses** - Run the main test loop.\n",
    "- **üìä Generate Report** - Save the results for analysis.\n",
    "\n",
    "## üìù How to Use This Notebook\n",
    "1. **‚ñ∂Ô∏è Run each cell in order** from top to bottom.\n",
    "2. **‚úèÔ∏è Modify the `qa_pairs` list** to test different questions and expected outcomes.\n",
    "3. **üìÇ Inspect the HTML report** at the end for detailed evaluation results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8689cf-bff8-4ffe-970f-0520e793925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import asyncio  # Needed for asynchronous operations\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# PyRIT Imports\n",
    "from pyrit.common import IN_MEMORY, initialize_pyrit\n",
    "from pyrit.prompt_target import OpenAIChatTarget, SteijnHTTPTarget\n",
    "from pyrit.score.evaluator import Evaluator\n",
    "from pyrit.orchestrator import SteijnPromptSendingOrchestrator\n",
    "from pyrit.common.text_helper import generate_dataset_report\n",
    "from pyrit.prompt_target import SteijnResponseParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbca6386-0660-45f2-bc4f-8b97ea44c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an in-memory database for a clean testing environment\n",
    "initialize_pyrit(memory_db_type=IN_MEMORY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee11384-d777-422c-9ec3-197d60f7c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Define Member ID and API credentials\n",
    "member_id = \"16611078\"\n",
    "url = os.getenv(\"STEIJN_ENDPOINT\") + member_id\n",
    "token = os.getenv(\"STEIJN_NONPRD_TOKEN\")\n",
    "\n",
    "# Define a raw HTTP POST request template with headers and a placeholder for the prompt\n",
    "start_chat_request_raw = f\"\"\"\n",
    "    POST {url}\n",
    "    Content-Type: application/json\n",
    "    X-Authorization: {token}\n",
    "    Accept: text/event-stream\n",
    "    x-rate-limiter-enabled: false\n",
    "    x-message-length-validation-enabled: false\n",
    "    x-user-message-evaluator-enabled: true\n",
    "\n",
    "{{\n",
    "    \"data\": \"{{PROMPT}}\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a0c12b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: regression ‚Üí 105 cases loaded\n",
      "Evaluator: general\n"
     ]
    }
   ],
   "source": [
    "from pyrit.loaders.test_data_loader import load_test_data\n",
    "\n",
    "# === Dataset Configuration ===\n",
    "DATASET_PATH = \"tests/data/dataset\"\n",
    "available_datasets = {\n",
    "    \"general\": \"qa_general_dataset.yaml\",\n",
    "    \"products\": \"qa_products_dataset.yaml\",\n",
    "    \"non_food\": \"qa_non_food_products_dataset.yaml\",\n",
    "    \"recipes\": \"qa_recipes_dataset.yaml\",\n",
    "    \"health\": \"qa_health_dataset.yaml\",\n",
    "    \"conversational\": \"qa_conversational_dataset.yaml\",\n",
    "    \"regression\": \"qa_regression_dataset.yaml\"\n",
    "}\n",
    "\n",
    "# === Evaluator Configuration ===\n",
    "available_evaluators = {\n",
    "    \"general\": \"assets/AH_Evaluators/ah_assistant/qa_general.yaml\",\n",
    "    \"products\": \"assets/AH_Evaluators/ah_assistant/qa_products.yaml\",\n",
    "    \"recipes\": \"assets/AH_Evaluators/ah_assistant/qa_recipes.yaml\",\n",
    "    \"relevance\": \"assets/AH_Evaluators/relevance_evaluator.yaml\",\n",
    "}\n",
    "\n",
    "# Select which dataset and evaluator you want to test\n",
    "selected_dataset = \"regression\"\n",
    "selected_evaluator = \"general\"\n",
    "\n",
    "# Load the dataset\n",
    "current_dataset = available_datasets[selected_dataset]\n",
    "qa_pairs = load_test_data(f\"{DATASET_PATH}/{current_dataset}\")\n",
    "\n",
    "# Set evaluator path\n",
    "evaluator_path = available_evaluators[selected_evaluator]\n",
    "\n",
    "# Preview laodeded data\n",
    "print(f\"Dataset: {selected_dataset} ‚Üí {len(qa_pairs)} cases loaded\")\n",
    "print(f\"Evaluator: {selected_evaluator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40044d8c-e475-4967-88ea-cf2ebd74ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HTTP target that sends prompts using the defined request template.\n",
    "http_prompt_target = SteijnHTTPTarget(\n",
    "    http_request=start_chat_request_raw,\n",
    "    prompt_regex_string=\"{PROMPT}\",\n",
    "    timeout=60.0,\n",
    "    callback_function=SteijnResponseParser.parse_response\n",
    ")\n",
    "\n",
    "# Create an evaluator that uses a YAML configuration for scoring suggestions.\n",
    "scorer = Evaluator(\n",
    "    chat_target=OpenAIChatTarget(),\n",
    "    evaluator_yaml_path=Path(evaluator_path),\n",
    "    scorer_type=\"float_scale\"\n",
    ")\n",
    "\n",
    "# Create the orchestrator for sending prompts and evaluating responses.\n",
    "orchestrator = SteijnPromptSendingOrchestrator(\n",
    "    objective_target=http_prompt_target,\n",
    "    scorers=[scorer],\n",
    "    batch_size=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b00abbd-bc60-41c3-bd36-db9d52dfbcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_report(results, execution_time):\n",
    "    # Define the report directory path and create it if it doesn't exist.\n",
    "    report_dir = Path(\"tests/E2E/reports/DataSet\").resolve()\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create a timestamp string\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Construct the filename with timestamp after the extension\n",
    "    filename = f\"{selected_dataset}_dataset_report_{timestamp}.html\"\n",
    "    \n",
    "    generate_dataset_report(\n",
    "        results=results,\n",
    "        save_path=report_dir / filename,\n",
    "        description=\"Mixed evaluation of single-turn and multi-turn prompt responses.\",\n",
    "        execution_time=execution_time,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd1a35e1-aec9-474e-8712-4c361c25365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # Start the timer before sending prompts.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Send the list of prompts asynchronously.\n",
    "    await orchestrator.send_qa_pairs_async(qa_pairs)  \n",
    "    \n",
    "    results = orchestrator.get_all_chat_results()  # Or use your combined method.\n",
    "\n",
    "    # Calculate the total execution time.\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    await generate_report(results, execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c5ff1ce-5f71-421d-94e5-570ad8b5de16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread ID not found in the first turn's response. Aborting this conversation.\n",
      "Thread ID not found in the first turn's response. Aborting this conversation.\n",
      "Retrying Prompt:Ik wil een vegetarisch BBQ houden met vrienden\n",
      "Thread ID not found in the first turn's response. Aborting this conversation.\n",
      "Thread ID not found in the first turn's response. Aborting this conversation.\n",
      "Thread ID not found in the first turn's response. Aborting this conversation.\n",
      "Thread ID not found in the first turn's response. Aborting this conversation.\n",
      "\n",
      "‚úÖ Dataset report saved to: /Users/denizdalkilic/Documents/Forks/PyRIT/tests/E2E/reports/DataSet/regression_dataset_report_20250410_103142.html\n"
     ]
    }
   ],
   "source": [
    "await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
